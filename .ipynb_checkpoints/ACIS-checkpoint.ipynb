{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACIS Car Insurance Analytics Solution\n",
    "\n",
    "This notebook implements a complete analytical pipeline for ACIS's car insurance \n",
    "data, covering all phases from data loading to predictive modeling and business insights.\n",
    "\n",
    "**Project Phases:**\n",
    "1. Data Loading & Preprocessing\n",
    "2. Exploratory Data Analysis (EDA)\n",
    "3. Hypothesis Testing\n",
    "4. Machine Learning Modeling\n",
    "5. Reporting & Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initial Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import shap\n",
    "import gc\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure display settings\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('mode.chained_assignment', None)  # Disable chained assignment warning\n",
    "\n",
    "# Visualization style\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess insurance data with robust error handling\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the insurance claims CSV file\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Cleaned and preprocessed DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"‚è≥ Loading and preprocessing data...\")\n",
    "        \n",
    "        # Initialize data containers\n",
    "        data = {\n",
    "            'policyid': [], 'gender': [], 'country': [], 'province': [],\n",
    "            'postalcode': [], 'vehicletype': [], 'registrationyear': [],\n",
    "            'make': [], 'model': [], 'suminsured': [], 'totalpremium': [],\n",
    "            'totalclaims': [], 'customvalueestimate': []\n",
    "        }\n",
    "        \n",
    "        # Define conversion functions\n",
    "        def safe_convert_float(x):\n",
    "            \"\"\"Convert European formatted numbers to float\"\"\"\n",
    "            try:\n",
    "                if pd.isna(x) or str(x).strip() == '':\n",
    "                    return np.nan\n",
    "                return float(str(x).replace('.','').replace(',','.'))\n",
    "            except:\n",
    "                return np.nan\n",
    "        \n",
    "        def safe_convert_int(x):\n",
    "            \"\"\"Convert European formatted numbers to integer\"\"\"\n",
    "            try:\n",
    "                if pd.isna(x) or str(x).strip() == '':\n",
    "                    return np.nan\n",
    "                return int(float(str(x).replace('.','').replace(',','.')))\n",
    "            except:\n",
    "                return np.nan\n",
    "        \n",
    "        # Process file line by line\n",
    "        with open(file_path, 'r', encoding='ISO-8859-1') as f:\n",
    "            reader = csv.reader(f, delimiter='|')\n",
    "            header = [col.strip().lower() for col in next(reader)]\n",
    "            \n",
    "            # Get column indices\n",
    "            col_indices = {col: header.index(col) if col in header else -1 for col in data.keys()}\n",
    "            \n",
    "            # Process all rows with progress bar\n",
    "            for row in tqdm(reader, desc=\"Processing rows\"):\n",
    "                try:\n",
    "                    for col in data.keys():\n",
    "                        if col_indices[col] != -1:\n",
    "                            val = row[col_indices[col]]\n",
    "                            if col in ['suminsured', 'totalpremium', 'totalclaims', 'customvalueestimate']:\n",
    "                                data[col].append(safe_convert_float(val))\n",
    "                            elif col == 'registrationyear':\n",
    "                                data[col].append(safe_convert_int(val))\n",
    "                            else:\n",
    "                                data[col].append(val)\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        \n",
    "        # Create DataFrame from collected data\n",
    "        df = pd.DataFrame({k: v for k, v in data.items() if len(v) > 0})\n",
    "        \n",
    "        # Data Quality Report\n",
    "        print(\"\\nüîç Data Quality Report:\")\n",
    "        print(f\"Initial Records: {len(df):,}\")\n",
    "        print(\"\\nMissing Values:\")\n",
    "        print(df.isnull().sum())\n",
    "        print(f\"\\nDuplicate Records: {df.duplicated().sum()}\")\n",
    "        \n",
    "        # Handle missing values using proper pandas methods\n",
    "        if 'customvalueestimate' in df.columns and 'suminsured' in df.columns:\n",
    "            df = df.assign(customvalueestimate=df['customvalueestimate'].fillna(df['suminsured']))\n",
    "        \n",
    "        # Drop rows with missing essential values\n",
    "        essential_cols = ['totalpremium', 'totalclaims']\n",
    "        df = df.dropna(subset=[col for col in essential_cols if col in df.columns])\n",
    "        \n",
    "        # Feature Engineering\n",
    "        current_year = datetime.now().year\n",
    "        if 'registrationyear' in df.columns:\n",
    "            df['vehicle_age'] = current_year - df['registrationyear']\n",
    "        \n",
    "        if 'totalpremium' in df.columns and 'totalclaims' in df.columns:\n",
    "            df['loss_ratio'] = np.where(\n",
    "                df['totalpremium'] > 0,\n",
    "                df['totalclaims'] / df['totalpremium'],\n",
    "                0\n",
    "            )\n",
    "            df['profit_margin'] = df['totalpremium'] - df['totalclaims']\n",
    "        \n",
    "        # Optimize data types\n",
    "        numeric_cols = ['suminsured', 'totalpremium', 'totalclaims', 'customvalueestimate', 'vehicle_age']\n",
    "        for col in numeric_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce').astype('float32')\n",
    "        \n",
    "        categorical_cols = ['gender', 'country', 'province', 'vehicletype', 'make', 'model']\n",
    "        for col in categorical_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype('category')\n",
    "        \n",
    "        # Remove duplicates\n",
    "        df = df.drop_duplicates()\n",
    "        \n",
    "        print(f\"\\n‚úÖ Final dataset: {len(df):,} records\")\n",
    "        print(f\"üìä Memory usage: {df.memory_usage(deep=True).sum()/1024/1024:.2f} MB\")\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in data loading: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_eda(df):\n",
    "    \"\"\"\n",
    "    Perform comprehensive exploratory data analysis\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Preprocessed insurance data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    bool\n",
    "        True if EDA completed successfully, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"\\nüìä Performing Exploratory Data Analysis...\")\n",
    "        \n",
    "        # 1. Data Structure Inspection\n",
    "        print(\"\\n=== DATA STRUCTURE ===\")\n",
    "        print(df.info())\n",
    "        \n",
    "        # 2. Descriptive Statistics\n",
    "        print(\"\\n=== DESCRIPTIVE STATISTICS ===\")\n",
    "        print(df.describe(include=[np.number]))\n",
    "        \n",
    "        # 3. Categorical Variable Analysis\n",
    "        print(\"\\n=== CATEGORICAL DISTRIBUTIONS ===\")\n",
    "        for col in df.select_dtypes(include='category').columns:\n",
    "            print(f\"\\n{col} value counts:\")\n",
    "            print(df[col].value_counts(dropna=False).head(10))\n",
    "        \n",
    "        # 4. Univariate Analysis - Visualizations\n",
    "        plt.figure(figsize=(18, 12))\n",
    "        \n",
    "        # Premium Distribution\n",
    "        plt.subplot(2, 2, 1)\n",
    "        if 'totalpremium' in df.columns:\n",
    "            sns.histplot(df['totalpremium'], bins=50, kde=True)\n",
    "            plt.title('Premium Distribution', fontsize=14)\n",
    "            plt.xlabel('Premium Amount')\n",
    "            plt.ylabel('Frequency')\n",
    "        \n",
    "        # Loss Ratio Distribution\n",
    "        plt.subplot(2, 2, 2)\n",
    "        if 'loss_ratio' in df.columns:\n",
    "            sns.histplot(df['loss_ratio'], bins=50, kde=True)\n",
    "            plt.title('Loss Ratio Distribution', fontsize=14)\n",
    "            plt.xlabel('Loss Ratio')\n",
    "            plt.ylabel('Frequency')\n",
    "        \n",
    "        # Vehicle Age Distribution\n",
    "        plt.subplot(2, 2, 3)\n",
    "        if 'vehicle_age' in df.columns:\n",
    "            sns.histplot(df['vehicle_age'], bins=20, kde=True)\n",
    "            plt.title('Vehicle Age Distribution', fontsize=14)\n",
    "            plt.xlabel('Vehicle Age (years)')\n",
    "            plt.ylabel('Frequency')\n",
    "        \n",
    "        # Claims Distribution\n",
    "        plt.subplot(2, 2, 4)\n",
    "        if 'totalclaims' in df.columns:\n",
    "            sns.histplot(df['totalclaims'], bins=50, kde=True)\n",
    "            plt.title('Claims Distribution', fontsize=14)\n",
    "            plt.xlabel('Claim Amount')\n",
    "            plt.ylabel('Frequency')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 5. Bivariate/Multivariate Analysis\n",
    "        print(\"\\n=== RISK ANALYSIS BY SEGMENT ===\")\n",
    "        \n",
    "        # By Province Analysis\n",
    "        if 'province' in df.columns and 'loss_ratio' in df.columns:\n",
    "            print(\"\\nTop 5 Provinces by Loss Ratio:\")\n",
    "            province_loss = df.groupby('province', observed=True)['loss_ratio'].mean()\\\n",
    "                            .sort_values(ascending=False).head(5)\n",
    "            print(province_loss)\n",
    "            \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            sns.barplot(x=province_loss.values, y=province_loss.index)\n",
    "            plt.title('Top 5 Provinces by Loss Ratio', fontsize=14)\n",
    "            plt.xlabel('Average Loss Ratio')\n",
    "            plt.ylabel('Province')\n",
    "            plt.show()\n",
    "        \n",
    "        # Vehicle Type Analysis\n",
    "        if 'vehicletype' in df.columns:\n",
    "            print(\"\\nVehicle Type Analysis:\")\n",
    "            vehicle_stats = df.groupby('vehicletype', observed=True)\\\n",
    "                            .agg({'totalpremium':'mean', 'loss_ratio':'mean', 'profit_margin':'mean'})\\\n",
    "                            .sort_values('loss_ratio', ascending=False)\n",
    "            print(vehicle_stats)\n",
    "            \n",
    "            plt.figure(figsize=(14, 7))\n",
    "            sns.boxplot(x='vehicletype', y='totalpremium', data=df)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.title('Premium Distribution by Vehicle Type', fontsize=14)\n",
    "            plt.xlabel('Vehicle Type')\n",
    "            plt.ylabel('Premium Amount')\n",
    "            plt.show()\n",
    "        \n",
    "        # Vehicle Age Group Analysis\n",
    "        if 'vehicle_age' in df.columns:\n",
    "            print(\"\\nVehicle Age Analysis:\")\n",
    "            df['age_group'] = pd.cut(df['vehicle_age'], \n",
    "                                   bins=[0, 5, 10, 15, 20, 100],\n",
    "                                   labels=['0-5', '6-10', '11-15', '16-20', '20+'])\n",
    "            \n",
    "            age_stats = df.groupby('age_group', observed=True)\\\n",
    "                        .agg({'totalpremium':'mean', 'loss_ratio':'mean', 'profit_margin':'mean'})\n",
    "            print(age_stats)\n",
    "            \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            sns.barplot(x='age_group', y='loss_ratio', data=df, estimator=np.mean)\n",
    "            plt.title('Average Loss Ratio by Vehicle Age Group', fontsize=14)\n",
    "            plt.xlabel('Vehicle Age Group (years)')\n",
    "            plt.ylabel('Average Loss Ratio')\n",
    "            plt.show()\n",
    "        \n",
    "        # Correlation Analysis\n",
    "        numeric_df = df.select_dtypes(include=[np.number])\n",
    "        if not numeric_df.empty:\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "            plt.title('Feature Correlation Matrix', fontsize=14)\n",
    "            plt.show()\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in EDA: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_hypothesis_tests(df):\n",
    "    \"\"\"\n",
    "    Perform statistical hypothesis tests to validate risk/profitability differences\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Preprocessed insurance data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    bool\n",
    "        True if tests completed successfully, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"\\nüî¨ Performing Hypothesis Tests...\")\n",
    "        \n",
    "        # 1. Test loss ratio differences between provinces\n",
    "        if 'province' in df.columns and 'loss_ratio' in df.columns:\n",
    "            provinces = df['province'].value_counts().nlargest(5).index.tolist()\n",
    "            samples = [df[df['province'] == prov]['loss_ratio'] for prov in provinces]\n",
    "            \n",
    "            # ANOVA test\n",
    "            print(\"\\nANOVA Test for Loss Ratio Across Top 5 Provinces:\")\n",
    "            f_val, p_val = stats.f_oneway(*samples)\n",
    "            print(f\"F-statistic: {f_val:.2f}, p-value: {p_val:.4f}\")\n",
    "            \n",
    "            # Pairwise t-tests with Bonferroni correction\n",
    "            print(\"\\nPairwise T-tests (Bonferroni corrected):\")\n",
    "            from itertools import combinations\n",
    "            alpha = 0.05\n",
    "            n_comparisons = len(list(combinations(provinces, 2)))\n",
    "            bonferroni_alpha = alpha / n_comparisons\n",
    "            \n",
    "            for prov1, prov2 in combinations(provinces, 2):\n",
    "                t_val, p_val = stats.ttest_ind(\n",
    "                    df[df['province'] == prov1]['loss_ratio'],\n",
    "                    df[df['province'] == prov2]['loss_ratio'],\n",
    "                    equal_var=False\n",
    "                )\n",
    "                significant = \"***\" if p_val < bonferroni_alpha else \"\"\n",
    "                print(f\"{prov1} vs {prov2}: t-stat={t_val:.2f}, p-value={p_val:.4f}{significant}\")\n",
    "        \n",
    "        # 2. Test gender differences in loss ratio (Mann-Whitney U for non-normal)\n",
    "        if 'gender' in df.columns and 'loss_ratio' in df.columns:\n",
    "            genders = df['gender'].value_counts().nlargest(2).index.tolist()\n",
    "            if len(genders) == 2:\n",
    "                print(\"\\nMann-Whitney U Test for Gender Differences in Loss Ratio:\")\n",
    "                u_val, p_val = stats.mannwhitneyu(\n",
    "                    df[df['gender'] == genders[0]]['loss_ratio'],\n",
    "                    df[df['gender'] == genders[1]]['loss_ratio'],\n",
    "                    alternative='two-sided'\n",
    "                )\n",
    "                print(f\"U-statistic: {u_val:.2f}, p-value: {p_val:.4f}\")\n",
    "                print(\"Note: Null hypothesis - distributions are equal\")\n",
    "        \n",
    "        # 3. Test vehicle type differences in profitability\n",
    "        if 'vehicletype' in df.columns and 'profit_margin' in df.columns:\n",
    "            vehicle_types = df['vehicletype'].value_counts().nlargest(3).index.tolist()\n",
    "            print(\"\\nKruskal-Wallis Test for Profit Margin Across Vehicle Types:\")\n",
    "            samples = [df[df['vehicletype'] == vt]['profit_margin'] for vt in vehicle_types]\n",
    "            h_val, p_val = stats.kruskal(*samples)\n",
    "            print(f\"H-statistic: {h_val:.2f}, p-value: {p_val:.4f}\")\n",
    "            print(\"Note: Null hypothesis - all distributions are equal\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in hypothesis testing: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Machine Learning Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ml_models(df):\n",
    "    \"\"\"\n",
    "    Build and evaluate predictive models for claims prediction\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Preprocessed insurance data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict or None\n",
    "        Dictionary containing model results or None if failed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"\\nü§ñ Building Machine Learning Models...\")\n",
    "        \n",
    "        # Prepare data\n",
    "        features = ['vehicletype', 'vehicle_age', 'suminsured', 'province', 'postalcode']\n",
    "        target = 'totalclaims'\n",
    "        \n",
    "        # Filter for available columns\n",
    "        available_features = [f for f in features if f in df.columns]\n",
    "        if not available_features or target not in df.columns:\n",
    "            print(\"‚ùå Required columns missing for modeling\")\n",
    "            return None\n",
    "        \n",
    "        X = df[available_features]\n",
    "        y = df[target]\n",
    "        \n",
    "        # Preprocessing pipeline\n",
    "        numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        categorical_features = X.select_dtypes(include=['category']).columns.tolist()\n",
    "        \n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', StandardScaler(), numeric_features),\n",
    "                ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "            ])\n",
    "        \n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Model dictionary\n",
    "        models = {\n",
    "            \"Linear Regression\": LinearRegression(),\n",
    "            \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "            \"XGBoost\": xgb.XGBRegressor(objective='reg:squarederror', random_state=42, n_jobs=-1)\n",
    "        }\n",
    "        \n",
    "        # Evaluate models\n",
    "        results = {}\n",
    "        for name, model in models.items():\n",
    "            print(f\"\\nTraining {name}...\")\n",
    "            pipeline = Pipeline(steps=[\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('model', model)\n",
    "            ])\n",
    "            \n",
    "            pipeline.fit(X_train, y_train)\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            results[name] = {\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse,\n",
    "                'R2': r2,\n",
    "                'model': pipeline,\n",
    "                'y_test': y_test,\n",
    "                'y_pred': y_pred\n",
    "            }\n",
    "            \n",
    "            print(f\"{name} Performance:\")\n",
    "            print(f\"MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.2f}\")\n",
    "        \n",
    "        # Model Comparison Visualization\n",
    "        metrics_df = pd.DataFrame.from_dict({k: v for k, v in results.items() if k in ['MAE', 'RMSE', 'R2']}, \n",
    "                                           orient='index')\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        metrics_df[['MAE', 'RMSE']].plot(kind='bar', rot=0)\n",
    "        plt.title('Model Comparison: MAE and RMSE', fontsize=14)\n",
    "        plt.ylabel('Error Value')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(8, 5))\n",
    "        metrics_df['R2'].plot(kind='bar', rot=0)\n",
    "        plt.title('Model Comparison: R-squared', fontsize=14)\n",
    "        plt.ylabel('R-squared Value')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "        \n",
    "        # Feature importance for best model\n",
    "        best_model_name = max(results, key=lambda x: results[x]['R2'])\n",
    "        print(f\"\\nBest Model: {best_model_name} (R2: {results[best_model_name]['R2']:.2f})\")\n",
    "        \n",
    "        if 'Random Forest' in results:\n",
    "            print(\"\\nFeature Importance (Random Forest):\")\n",
    "            rf_model = results['Random Forest']['model'].named_steps['model']\n",
    "            feature_names = numeric_features + \\\n",
    "                          list(results['Random Forest']['model'].named_steps['preprocessor']\\\n",
    "                          .named_transformers_['cat'].get_feature_names_out(categorical_features))\n",
    "            \n",
    "            importances = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Importance': rf_model.feature_importances_\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "            \n",
    "            print(importances.head(10))\n",
    "            \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            sns.barplot(x='Importance', y='Feature', data=importances.head(10))\n",
    "            plt.title('Top 10 Important Features (Random Forest)', fontsize=14)\n",
    "            plt.show()\n",
    "        \n",
    "        # SHAP analysis for XGBoost\n",
    "        if 'XGBoost' in results:\n",
    "            print(\"\\nSHAP Analysis (XGBoost):\")\n",
    "            xgb_model = results['XGBoost']['model'].named_steps['model']\n",
    "            preprocessor = results['XGBoost']['model'].named_steps['preprocessor']\n",
    "            \n",
    "            # Get processed feature names\n",
    "            processed_features = numeric_features + \\\n",
    "                               list(preprocessor.named_transformers_['cat']\\\n",
    "                               .get_feature_names_out(categorical_features))\n",
    "            \n",
    "            # Get processed test data\n",
    "            X_test_processed = preprocessor.transform(X_test)\n",
    "            \n",
    "            # SHAP analysis\n",
    "            explainer = shap.Explainer(xgb_model)\n",
    "            shap_values = explainer(X_test_processed)\n",
    "            \n",
    "            plt.figure()\n",
    "            shap.summary_plot(shap_values, X_test_processed, feature_names=processed_features, plot_type='bar')\n",
    "            plt.title('Feature Importance (SHAP)', fontsize=14)\n",
    "            plt.show()\n",
    "            \n",
    "            # Individual SHAP explanation\n",
    "            plt.figure()\n",
    "            shap.plots.waterfall(shap_values[0], max_display=10)\n",
    "            plt.title('Individual Prediction Explanation', fontsize=14)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in modeling: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Business Insights Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_insights(df, model_results):\n",
    "    \"\"\"\n",
    "    Generate actionable business insights from analysis\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Preprocessed insurance data\n",
    "    model_results : dict\n",
    "        Results from machine learning modeling\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    bool\n",
    "        True if insights generated successfully, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"\\nüìà Generating Business Insights...\")\n",
    "        \n",
    "        # 1. Executive Summary\n",
    "        print(\"\\n=== EXECUTIVE SUMMARY ===\")\n",
    "        avg_loss_ratio = df['loss_ratio'].mean()\n",
    "        avg_profit_margin = df['profit_margin'].mean()\n",
    "        \n",
    "        print(f\"\\nOverall Portfolio Metrics:\")\n",
    "        print(f\"Average Loss Ratio: {avg_loss_ratio:.2%}\")\n",
    "        print(f\"Average Profit Margin: ${avg_profit_margin:,.2f}\")\n",
    "        print(f\"Total Policies Analyzed: {len(df):,}\")\n",
    "        \n",
    "        # 2. Key Findings\n",
    "        print(\"\\n=== KEY FINDINGS ===\")\n",
    "        \n",
    "        # Risk Analysis\n",
    "        if 'province' in df.columns:\n",
    "            high_risk_provinces = df.groupby('province', observed=True)['loss_ratio'].mean()\\\n",
    "                                  .nlargest(3).index.tolist()\n",
    "            print(f\"\\nHighest Risk Provinces: {', '.join(high_risk_provinces)}\")\n",
    "        \n",
    "        if 'vehicletype' in df.columns:\n",
    "            high_risk_vehicles = df.groupby('vehicletype', observed=True)['loss_ratio'].mean()\\\n",
    "                                .nlargest(2).index.tolist()\n",
    "            print(f\"Highest Risk Vehicle Types: {', '.join(high_risk_vehicles)}\")\n",
    "        \n",
    "        # Profitability Analysis\n",
    "        if 'province' in df.columns:\n",
    "            profitable_provinces = df.groupby('province', observed=True)['profit_margin'].mean()\\\n",
    "                                  .nlargest(3).index.tolist()\n",
    "            print(f\"\\nMost Profitable Provinces: {', '.join(profitable_provinces)}\")\n",
    "        \n",
    "        if 'vehicletype' in df.columns:\n",
    "            profitable_vehicles = df.groupby('vehicletype', observed=True)['profit_margin'].mean()\\\n",
    "                                .nlargest(2).index.tolist()\n",
    "            print(f\"Most Profitable Vehicle Types: {', '.join(profitable_vehicles)}\")\n",
    "        \n",
    "        # 3. Actionable Recommendations\n",
    "        print(\"\\n=== RECOMMENDATIONS ===\")\n",
    "        print(\"\\n1. Pricing Strategy:\")\n",
    "        print(\"- Implement risk-based pricing for high-risk provinces/vehicle types\")\n",
    "        print(\"- Offer competitive premiums for low-risk segments to attract more customers\")\n",
    "        print(\"- Consider usage-based insurance models for high-risk groups\")\n",
    "        \n",
    "        print(\"\\n2. Marketing Strategy:\")\n",
    "        print(\"- Target marketing campaigns toward most profitable segments\")\n",
    "        print(\"- Develop retention programs for low-risk, high-value customers\")\n",
    "        print(\"- Create educational content for high-risk groups to improve driving behavior\")\n",
    "        \n",
    "        print(\"\\n3. Risk Management:\")\n",
    "        print(\"- Implement stricter underwriting for highest-risk categories\")\n",
    "        print(\"- Consider partnerships with repair shops in high-risk areas\")\n",
    "        print(\"- Develop telematics programs to better assess driver risk\")\n",
    "        \n",
    "        if model_results:\n",
    "            best_model = max(model_results, key=lambda x: model_results[x]['R2'])\n",
    "            print(f\"\\n4. Predictive Modeling Insights (Best Model: {best_model}):\")\n",
    "            print(\"- Use model predictions to set optimal premiums\")\n",
    "            print(\"- Incorporate top predictive features into underwriting criteria\")\n",
    "            print(\"- Implement automated pricing adjustments based on model outputs\")\n",
    "        \n",
    "        # 4. Implementation Roadmap\n",
    "        print(\"\\n=== IMPLEMENTATION ROADMAP ===\")\n",
    "        print(\"\\nQuarter 1:\")\n",
    "        print(\"- Implement risk-based pricing for top 3 high-risk provinces\")\n",
    "        print(\"- Launch targeted marketing campaign for most profitable segments\")\n",
    "        \n",
    "        print(\"\\nQuarter 2:\")\n",
    "        print(\"- Roll out model-driven pricing adjustments\")\n",
    "        print(\"- Establish partnerships with repair networks\")\n",
    "        \n",
    "        print(\"\\nQuarter 3:\")\n",
    "        print(\"- Evaluate impact of pricing changes\")\n",
    "        print(\"- Expand telematics program based on initial results\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in generating insights: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Main Execution Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function for the analysis pipeline\"\"\"\n",
    "    print(\"=== ACIS CAR INSURANCE ANALYTICS ===\")\n",
    "    print(\"Comprehensive Risk and Profitability Analysis\\n\")\n",
    "    \n",
    "    # Phase 1: Data Loading & Preprocessing\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PHASE 1: DATA LOADING & PREPROCESSING\")\n",
    "    print(\"=\"*50)\n",
    "    df = load_and_preprocess_data('insurance_claims.csv')\n",
    "    if df is None:\n",
    "        return\n",
    "    \n",
    "    # Phase 2: Exploratory Data Analysis\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PHASE 2: EXPLORATORY DATA ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    if not perform_eda(df):\n",
    "        return\n",
    "    \n",
    "    # Phase 3: Hypothesis Testing\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PHASE 3: HYPOTHESIS TESTING\")\n",
    "    print(\"=\"*50)\n",
    "    if not perform_hypothesis_tests(df):\n",
    "        return\n",
    "    \n",
    "    # Phase 4: Machine Learning Modeling\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PHASE 4: MACHINE LEARNING MODELING\")\n",
    "    print(\"=\"*50)\n",
    "    model_results = build_ml_models(df)\n",
    "    \n",
    "    # Phase 5: Reporting & Insights\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PHASE 5: REPORTING & INSIGHTS\")\n",
    "    print(\"=\"*50)\n",
    "    generate_insights(df, model_results)\n",
    "    \n",
    "    print(\"\\n‚úÖ Analysis completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Analysis\n",
    "\n",
    "Execute the main function to run the complete analysis pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}